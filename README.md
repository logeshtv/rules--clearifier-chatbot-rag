# RAG Chatbot API ğŸ¤–

A production-ready RAG (Retrieval-Augmented Generation) chatbot API with vector search capabilities using Qdrant, Xenova embeddings, and OpenAI LLM.

## âœ¨ Features

- ğŸš€ **RESTful API** with Express.js
- ğŸ’¬ **Streaming Chat Responses** with context-aware conversations
- ğŸ“„ **Document Processing** (PDF & TXT files)
- ğŸ” **Vector Search** with Qdrant
- ğŸ§  **Local Embeddings** with Xenova (no API costs!)
- ğŸ’¾ **Chat History** with pagination
- ğŸ” **Password-Protected Uploads**
- ğŸ¨ **Simple Web UI** included
- ğŸ³ **Docker Support** for easy deployment

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web UI        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Express API   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Controllers    â”‚
â”‚  Services       â”‚
â”‚  Utils          â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
      â”‚       â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ Qdrant â”‚ â”‚ Xenova â”‚
â”‚Vector  â”‚ â”‚Embedderâ”‚
â”‚   DB   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ API Endpoints

### Chat Endpoints
- `POST /api/chat` - Send chat message (streaming response)
- `GET /api/chat/history/:userId` - Get chat history (paginated)
- `DELETE /api/chat/history/:userId` - Clear chat history
- `GET /api/chat/stats` - Get chat statistics

### Upload Endpoints
- `POST /api/upload/document` - Upload PDF or TXT file
- `POST /api/upload/text` - Upload raw text
- `GET /api/upload/stats` - Get upload statistics

### System Endpoints
- `GET /api/health` - Health check
- `GET /api/info` - System information

## ğŸš€ Quick Start

### Option 1: Docker (Recommended)

1. **Clone and configure**
   ```bash
   cd rules-clarify-rag
   cp .env.example .env
   # Edit .env and add your OPENAI_API_KEY
   ```

2. **Start services**
   ```bash
   docker-compose up -d
   ```

3. **Access the application**
   - UI: http://localhost:3000
   - API: http://localhost:3000/api
   - Qdrant: http://localhost:6333

### Option 2: Local Development

1. **Install dependencies**
   ```bash
   npm install
   ```

2. **Start Qdrant (using Docker)**
   ```bash
   docker run -p 6333:6333 qdrant/qdrant:latest
   ```

3. **Configure environment**
   ```bash
   cp .env.example .env
   # Edit .env and configure settings
   ```

4. **Start the server**
   ```bash
   npm start
   # or for development with auto-reload
   npm run dev
   ```

## ğŸ”§ Configuration

### Environment Variables

```env
# Server
PORT=3000
NODE_ENV=development

# Upload Password
UPLOAD_PASSWORD=adrigdeva

# Qdrant
QDRANT_URL=http://localhost:6333
QDRANT_COLLECTION=rag_documents

# Embedding (Xenova - Local)
EMBEDDING_METHOD=XENOVA
XENOVA_MODEL=Xenova/all-MiniLM-L6-v2
XENOVA_DIMENSIONS=384

# LLM (OpenAI)
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=gpt-4o-mini

# RAG Settings
RAG_TOP_K=5
RAG_MIN_SCORE=0.5
RAG_CONTEXT_WINDOW=10
```

## ğŸ“– Usage Examples

### 1. Upload Documents

**Using UI:**
- Navigate to http://localhost:3000
- Enter password: `adrigdeva`
- Upload PDF or paste text

**Using API:**
```bash
# Upload PDF
curl -X POST http://localhost:3000/api/upload/document \
  -F "password=adrigdeva" \
  -F "file=@document.pdf"

# Upload Text
curl -X POST http://localhost:3000/api/upload/text \
  -H "Content-Type: application/json" \
  -d '{
    "password": "adrigdeva",
    "text": "Your text content here...",
    "source": "manual-input"
  }'
```

### 2. Chat with Bot

```bash
curl -X POST http://localhost:3000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "userId": "user-001",
    "query": "What is the main topic of the documents?"
  }'
```

### 3. Get Chat History

```bash
curl http://localhost:3000/api/chat/history/user-001?page=1&pageSize=20
```

## ğŸ“ Project Structure

```
rules-clarify-rag/
â”œâ”€â”€ config.js                 # Configuration management
â”œâ”€â”€ index.js                  # Express app entry point
â”œâ”€â”€ package.json
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â”‚
â”œâ”€â”€ controllers/              # Request handlers
â”‚   â”œâ”€â”€ chat.controller.js
â”‚   â”œâ”€â”€ upload.controller.js
â”‚   â””â”€â”€ system.controller.js
â”‚
â”œâ”€â”€ services/                 # Business logic
â”‚   â”œâ”€â”€ qdrant.service.js     # Vector DB operations
â”‚   â”œâ”€â”€ embedding.service.js  # Embedding generation
â”‚   â”œâ”€â”€ llm.service.js        # LLM interactions
â”‚   â”œâ”€â”€ chatHistory.service.js
â”‚   â””â”€â”€ cache.service.js
â”‚
â”œâ”€â”€ utils/                    # Utility functions
â”‚   â”œâ”€â”€ textProcessing.js
â”‚   â”œâ”€â”€ documentParser.js
â”‚   â””â”€â”€ validation.js
â”‚
â”œâ”€â”€ routes/                   # API routes
â”‚   â”œâ”€â”€ index.js
â”‚   â”œâ”€â”€ chat.routes.js
â”‚   â”œâ”€â”€ upload.routes.js
â”‚   â””â”€â”€ system.routes.js
â”‚
â”œâ”€â”€ middlewares/              # Express middlewares
â”‚   â””â”€â”€ index.js
â”‚
â””â”€â”€ public/                   # Frontend UI
    â”œâ”€â”€ index.html
    â”œâ”€â”€ styles.css
    â””â”€â”€ app.js
```

## ğŸ” How It Works

1. **Document Upload**
   - User uploads PDF/TXT or pastes text
   - Text is chunked into smaller pieces (~500 chars)
   - Each chunk is embedded using Xenova (local, free)
   - Embeddings stored in Qdrant vector database

2. **Chat Query**
   - User sends a question
   - Question is embedded
   - Top K similar chunks retrieved from Qdrant
   - Context + chat history sent to OpenAI LLM
   - Response streamed back to user
   - Conversation saved to history

3. **Chat History**
   - Each conversation stored with userId
   - Recent context used for follow-up questions
   - Paginated history retrieval
   - Clear history option

## ğŸ³ Docker Commands

```bash
# Build and start
npm run docker:build
npm run docker:up

# View logs
npm run docker:logs

# Stop services
npm run docker:down

# Restart
docker-compose restart rag-api
```

## ğŸ”’ Security Notes

- Upload password: `adrigdeva` (change in production!)
- Store your OpenAI API key securely in `.env`
- Never commit `.env` to version control
- Use HTTPS in production
- Implement rate limiting for production use

## ğŸ¯ Performance Tips

1. **Xenova Model**: `all-MiniLM-L6-v2` is fast and efficient (384 dimensions)
2. **Chunk Size**: 500 chars works well for most documents
3. **Top K**: 5 chunks usually provides enough context
4. **Cache**: Embeddings are cached to avoid recomputation
5. **Docker**: Use volumes for persistent storage

## ğŸ› ï¸ Troubleshooting

**Xenova model loading slow?**
- First load downloads model (~25MB)
- Cached in `/app/.cache` for subsequent uses

**Qdrant connection failed?**
- Ensure Qdrant is running: `docker ps | grep qdrant`
- Check URL in `.env` matches your setup

**Out of memory?**
- Reduce `CHUNK_SIZE` in `.env`
- Process fewer documents at once
- Increase Docker memory allocation

**Large file processing timeout?**
- Processing time scales with file size
- UI shows progress indicator
- Wait up to 10 minutes for large PDFs

## ğŸ“Š System Requirements

- **Node.js**: 18.0.0 or higher
- **RAM**: 2GB minimum (4GB recommended)
- **Disk**: 500MB for models and cache
- **Docker**: 20.10 or higher (if using Docker)

## ğŸ¤ Contributing

Contributions are welcome! Feel free to:
- Report bugs
- Suggest features
- Submit pull requests

## ğŸ“„ License

ISC

## ğŸ™ Acknowledgments

- [Qdrant](https://qdrant.tech/) - Vector database
- [Xenova Transformers](https://huggingface.co/Xenova) - Local embeddings
- [OpenAI](https://openai.com/) - LLM API
- [Express.js](https://expressjs.com/) - Web framework

---

Made with â¤ï¸ for RAG applications
