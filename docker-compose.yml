version: '3.8'

services:
  # Qdrant Vector Database
  qdrant:
    image: qdrant/qdrant:latest
    container_name: rag-qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_storage:/qdrant/storage
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
    restart: unless-stopped
    networks:
      - rag-network

  # RAG Chatbot API
  rag-api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rag-chatbot-api
    ports:
      - "3000:3000"
    environment:
      # Server
      - PORT=3000
      - NODE_ENV=production
      
      # Upload Password
      - UPLOAD_PASSWORD=adrigdeva
      
      # Qdrant
      - QDRANT_URL=http://qdrant:6333
      - QDRANT_COLLECTION=rag_documents
      
      # Embedding (Xenova - Local, No API Key Needed)
      - EMBEDDING_METHOD=XENOVA
      - XENOVA_MODEL=Xenova/all-MiniLM-L6-v2
      - XENOVA_DIMENSIONS=384
      
      # LLM (OpenAI - Replace with your API key)
      - LLM_PROVIDER=OPENAI
      - OPENAI_API_KEY=${OPENAI_API_KEY:-your-openai-api-key-here}
      - OPENAI_MODEL=gpt-4o-mini
      
      # RAG Configuration
      - RAG_TOP_K=5
      - RAG_MIN_SCORE=0.5
      - RAG_CONTEXT_WINDOW=10
      
      # Document Processing
      - CHUNK_SIZE=500
      - CHUNK_OVERLAP=50
      - MAX_FILE_SIZE=52428800
    volumes:
      - ./logs:/app/logs
      - transformers_cache:/app/.cache
    depends_on:
      - qdrant
    restart: unless-stopped
    networks:
      - rag-network
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3000/api/health', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)})"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

volumes:
  qdrant_storage:
    driver: local
  transformers_cache:
    driver: local

networks:
  rag-network:
    driver: bridge
